## 1.2 一个例子

为了指出这些问题的复杂性，让我们思考下面富有想象力的例子。假设有一个鱼肉食品加工厂想把鱼在传送带上根据物种自动分类。一个尝试性的想法是，把海鲈鱼和鲑鱼通过光学感知进行分类。我们装好相机，拍摄一些图片样例并注意到两种鱼的不同体型特征——长度，亮度，宽度，鱼鳍的数量和形状，嘴巴的位置等等——这些可以当作我们的分类器能依据的*特征*。我们也注意到图片的噪音和变化——包括亮度变化，传送带上鱼的不同位置，甚至有来自相机本身的电子传感器的影响。

实际上，海鲈鱼和鲑鱼的确有很多不同点，我们可以把它们看作拥有不同的*模型*——不同的描述，这是典型的数学形式。在模式分类中非常重要的目的和方法，就是假设这些模型的类别，处理敏感数据以消除噪音，以及任何能够选择模型时表现最好的敏感模式。任何可以促进这个目的的技术都应该被是模式识别系统工具的一员。

我们实现这项特殊工作的原型机应该像图1.1那样。首先，相机捕获到鱼的图片。接着，相机信号被随后的简单操作*预处理*，这一步不会丢失相关信息。实际上，我们可能会使用*分割*操作把鱼从其他鱼或者背景提取出来。来自单条鱼的信息被送往*特征提取器*，这个提取器可以通过测量某种“特征”或“属性”将数据量减少。最后，这些特征值被送往*分类器*，评估表现并对物种所属做出最后决策。

![图1.1](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.01.jpg)
> 图1.1

预处理器可能会自动调整亮度到平均值，或者传送带背景的边界线等等。现在让我们考虑鱼的图片可能会被如何分割，并考虑特征提取器和分类器应如何设计。假设鱼工厂的人告诉我们海鲈鱼通常比鲑鱼要长。这给了我们鱼的初试模型：海鲈鱼拥有典型的长度，并且长度通常大于鲑鱼的长度。于是长度变成了一个明显的特征，我们可能会试着观察鱼的长度 l 是否大于某个特定长度 l* 。为了选择 l*，我们可以获得一些*方案*或者不同鱼类*训练样本*，测量长度，然后观察结果。

假设我们完成了这些，并且得到了图1.2那样的直方图。图表明海鲈鱼一般来说比鲑鱼长，但很显然这个单一标准不够充分；无论我们怎么选取 l* 的值，都不能完全正确地仅根据长度来对海鲈鱼和鲑鱼分类。

![图1.2](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.02.jpg)
> 图1.2

结果并不乐观，让我们尝试另一种特征——鱼鳞的平均亮度。现在我们非常小心地排除了光照变化，因为光照变化会使模型模糊并影响我们的新分类器。由此得到直方图，如图1.3所示，这更容易实现分类了，两种鱼类被区分地很好。

![图1.3](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.03.jpg)
> 图1.3

到现在为止，我们默认假设我们的行为带来的结果的代价是相同的：把鲑鱼错当为鲈鱼和把鲈鱼当作鲑鱼的代价相同。通常来说是如此，但这在其他情景并不一定。举个例子，以鱼加工厂为例，我们知道消费者可以忍受在标着“海鲈鱼”的罐头里发现了鲑鱼的肉。但是他们非常不喜欢在标着“鲑鱼”的罐头里发现了海鲈鱼。如果我们想继续营业，那么我们必须调整我们的决策边界以避免激怒消费者，尽管这有可能会导致海鲈鱼罐头中出现更多鲑鱼肉。在这个例子中，我们需要把我们的决策边界 x* 移动到一个更小的光照值，以减少可能被误当作鲑鱼的海鲈鱼的数量（如图1.3所示）。假设有越多的消费者不喜欢在鲑鱼罐头发现海鲈鱼，那么我们就应该把决策边界 x* 移动到图1.3中值更小的位置去。

上述想法表明，有一个综合代价会和我们的最终决策相关，而我们的真正任务是指定一个决策规则（比如决策边界）来最小化这种代价。这就是*决策论*的主要任务，而决策论可以说是模式分类中最重要的子领域。

尽管我们知道，最终代价和我们的决策有关，并且选定了最佳决策边界 x* ，我们仍有可能得不到希望中的结果。我们的第一想法可能是找到不同的特征来对鱼进行分类。假设，没有其他某个更好的可视化单一特征，除了亮度以外。那么为了提升识别效果，我们必须采取多种特征同时参与。

对于其他特征，我们可能会注意到海鲈鱼通常比鲑鱼体型更宽。于是现在有了两种分类特征：亮度 x1 和宽度 x2。假设忽视这些特征的测量方法，我们发现特征提取器会把每条鱼映射到二维空间中的一个点或者特征向量 **x**。即 **x** = [x1, x2]。

那么问题变成如何把特征空间分成两个区域，在某个区域范围内都是海鲈鱼，另一个区域范围内的点都可视为鲑鱼。假设我们得到了如图1.4的测量结果。从图容易看出，可以根据这个规则分类：如果某条鱼的特征向量低于决策边界，就可以视为海鲈鱼；否则视为鲑鱼。

![图1.4](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.04.jpg)
> 图1.4

这个规则对于鱼的分类看起来似乎不错。除了亮度和宽度，我们还可以把形状参数加入进去，比如背鳍的顶角大小，或者眼睛的位置等等。但是我们怎么提前知道哪些特征是最有价值的呢？一些特征可能是冗余的：比如眼睛的颜色若和鱼的宽度相关，那么即便把眼睛颜色加入特征，最终分类表现也不会提升。过多的特征也会带来计算开销。

假设某些特征的测量方法比较昂贵，或者对于最终的分类性能提升甚微，让我们只能采用如图1.4所示的两个特征。如果我们的模型极其复杂，那么分类器的决策边界线将会比一条简单的直线复杂得多。在这种情况下，所有的训练模式都可以被完美区分，如图1.5所示。但是如果使用这种方案，那么对于新的模式，也就是鱼，我们似乎无法很好地进行分类。这就是 *泛化* 问题。像图1.5那样的复杂决策边界并没有好的泛化能力，因为它完全按照训练样本进行调整，而没有考虑到将来可能会出现的新的鱼的属性。

![图1.5](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.05.jpg)
> 图1.5

自然，我们会想着获取更多的训练样本以应对潜在的鱼的属性。然而在大多数模式识别问题中，我们能获取的样本有限。并且就算我们有非常多的数据，但我们如果按照图1.5的方式分类，得到一条相当复杂的决策边界，那么最后在新的样本模式上也只会表现得很糟糕。

于是我们回想着简化识别器，考虑即将可能出现的模型。实际上，如果在训练样本上分类表现稍差，却在新的模式表现很好时，那么这个模型是令人满意的。那么我们应该如何得到相对更简单的分类器呢？我们怎么让系统自动决定图1.6、图1.4、图1.5中的哪条曲线是最好的呢？假设我们能达到这种权衡，那我们又如何预测这个系统对于新模式的泛化能力呢？这些就是 *统计模式识别* 的主要问题。

![图1.6](https://yg-1255660153.cos.ap-chengdu.myqcloud.com/PatternClassification/F01.06.jpg)
> 图1.6

对于可能的模式，我们会使用完全不同的代价函数，这将会导致不同的结果。比如为了出售鱼子，可能要对鱼的性别进行分类。或者可能要把身体破损的鱼先宰杀等等。不同的决策任务可能需要不同的特征，并导致决策边界和最开始的鱼品种分类完全不同。这意味着我们的决策完全基于具体任务或者代价。

在鱼类样本中，我们需要仔细选择特征，因而得到鱼的 *表示* 用于分类。在模式识别问题中，一个好的表示通常影响最终的表现。有些情况下，可表示为实数向量，有时候是一列属性，有时候是一些关系和描述。对于分类，同一类中的表示距离较近，不同类的表示距离较远。我们还会想着用尽可能少的特征，以得到更简单的决策边界和容易训练的分类器。我们还会希望这些特征具有鲁棒性，对于某些噪声和误差不那么敏感。实际应用中，我们可能还需要分类器能运算地快一些，或者使用更少的电子元件，内存，或者处理步骤。

通常，如果数据越少，我们就需要进行越多的人工干预分析。比如在语音识别中，每个人对于“D”的发音就不完全一样。那么我们就需要更多的人工干预设计特征。

考虑椅子的分类，我们也难以找到同一类椅子的共同之处。

除了如此极端的例子以外，许多现实中的模式识别问题都会尝试加入一点人工特征，以达到更好的表现。比如在光学字符识别中，我们可以保证手写字符是一系列的笔画，并可以从传感器图像中复现笔画，并根据笔画推断出具体字符。

### 1.2.2 相关领域

模式分类不同于假设性检验。假设性检验通常只针对正负二元假设。而模式分类是从众多假设中找到可能性最高的那个假设。

模式分类在图像处理中也应用广泛。比如特征提取，在图像中找到亮度的峰值和谷值。

由于特征提取会损失部分信息，因此也可以用于计算机的联合存储器。
